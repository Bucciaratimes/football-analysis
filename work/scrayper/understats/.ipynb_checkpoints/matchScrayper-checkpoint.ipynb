{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter the match id:  17478\n",
      "please input match number. 35\n",
      "please input season. 2122\n",
      "please input target team. barcelona\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://understat.com/match/'\n",
    "match = str(input('Please enter the match id: '))\n",
    "url = base_url+match\n",
    "\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content, 'lxml')\n",
    "scripts = soup.find_all('script')\n",
    "\n",
    "strings = scripts[1].string\n",
    "\n",
    "index_start = strings.index(\"('\")+2\n",
    "index_end = strings.index(\"')\")\n",
    "json_data = strings[index_start:index_end]\n",
    "json_data = json_data.encode('utf8').decode('unicode_escape')\n",
    "data = json.loads(json_data)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "minute = []\n",
    "xG = []\n",
    "result = []\n",
    "team = []\n",
    "player = []\n",
    "shotType = []\n",
    "assist = []\n",
    "data_away = data['a']\n",
    "data_home = data['h']\n",
    "\n",
    "for index in range(len(data_home)):\n",
    "    for key in data_home[index]:\n",
    "        if key == 'X':\n",
    "            x.append(data_home[index][key])\n",
    "        if key == 'Y':\n",
    "            y.append(data_home[index][key])\n",
    "        if key == 'minute':\n",
    "            minute.append(data_home[index][key])\n",
    "        if key == 'h_team':\n",
    "            team.append(data_home[index][key])\n",
    "        if key == 'xG':\n",
    "            xG.append(data_home[index][key])\n",
    "        if key == 'result':\n",
    "            result.append(data_home[index][key])\n",
    "        if key == 'player':\n",
    "            player.append(data_home[index][key])\n",
    "        if key == 'player_assisted':\n",
    "            assist.append(data_home[index][key])\n",
    "        if key == 'shotType':\n",
    "            shotType.append(data_home[index][key])\n",
    "\n",
    "for index in range(len(data_away)):\n",
    "    for key in data_away[index]:\n",
    "        if key == 'X':\n",
    "            x.append(data_away[index][key])\n",
    "        elif key == 'Y':\n",
    "            y.append(data_away[index][key])\n",
    "        elif key == 'minute':\n",
    "            minute.append(data_away[index][key])\n",
    "        elif key == 'a_team':\n",
    "            team.append(data_away[index][key])\n",
    "        elif key == 'xG':\n",
    "            xG.append(data_away[index][key])\n",
    "        elif key == 'result':\n",
    "            result.append(data_away[index][key])\n",
    "        elif key == 'player':\n",
    "            player.append(data_away[index][key])\n",
    "        elif key == 'player_assisted':\n",
    "            assist.append(data_away[index][key])\n",
    "        elif key == 'shotType':\n",
    "            shotType.append(data_away[index][key])\n",
    "\n",
    "\n",
    "col_names = [ 'minute', 'team','player', 'x', 'y', 'xG', 'result', 'shotType','assist']\n",
    "df = pd.DataFrame([minute, team, player, x, y, xG ,result ,shotType, assist],index=col_names)\n",
    "df = df.T\n",
    "df = df.astype({'x':float, 'y':float, 'minute':int, 'xG':float,'player':str, 'shotType':str, 'assist':str,'team':str})\n",
    "\n",
    "comp = str(input(\"please input match number.\")) # ex #1\n",
    "season = str(input(\"please input season.\"))\n",
    "team = str(input(\"please input target team.\"))\n",
    "\n",
    "path = f'/work/assets/understats/{season}/{team}/'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "df.to_csv(path+\"#\"+comp+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://github.com/douglasbc/scraping-understat-dataset/tree/main/scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/scrayper/understats\n",
      "/work/scrayper/understats/datasets\n",
      "/work/scrayper/understats/scraping\n",
      "/work/scrayper/understats/scraping/teams_set\n"
     ]
    }
   ],
   "source": [
    "CWD = os.getcwd()\n",
    "DIR = os.path.join(CWD,\"datasets\")\n",
    "DIR_CODE = os.path.join(CWD,\"scraping\")\n",
    "DIR_TEAMS = os.path.join(CWD,\"scraping\", \"teams_set\")\n",
    "print(CWD)\n",
    "print(DIR)\n",
    "print(DIR_CODE)\n",
    "print(DIR_TEAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://understat.com/match/\"\n",
    "\n",
    "def scrape_script_tags(match_id):\n",
    "    \"\"\"\n",
    "    Takes a match_id (integer) to define a URL to be scraped.\n",
    "    Returns a ResultSet object, class implemented by BeautifulSoup, with all\n",
    "    the script tags in the URL.\n",
    "    \"\"\"\n",
    "    URL = BASE_URL + str(match_id)\n",
    "\n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    soup_scripts = soup.find_all(\"script\")\n",
    "    return soup_scripts\n",
    "\n",
    "\n",
    "def create_shots_list(soup_scripts):\n",
    "    \"\"\"\n",
    "    Takes a ResultSet and returns a list of dictionaries containing all shots\n",
    "    in a soccer match.\n",
    "    \"\"\"\n",
    "    shots_string = soup_scripts[1].string\n",
    "    start_index = shots_string.index(\"('\")+2\n",
    "    end_index = shots_string.index(\"')\")\n",
    "    json_string = shots_string[start_index:end_index]\n",
    "    json_string = json_string.encode(\"utf8\").decode(\"unicode_escape\")\n",
    "    shots_dict = json.loads(json_string)\n",
    "\n",
    "    shots_home = shots_dict[\"h\"]\n",
    "    shots_away = shots_dict[\"a\"]\n",
    "\n",
    "    shots_list = shots_home + shots_away\n",
    "    return shots_list\n",
    "\n",
    "\n",
    "def generate_shots_dataset(iterable):\n",
    "    \"\"\"\n",
    "    Takes an iterable object with matches_id. It is passed to a loop that\n",
    "    goes through all matches and scrapes the data. Sometimes a Connection Error\n",
    "    is raised when trying to scrape many matches at once, so might be a good\n",
    "    idea to scrape only a few hundred matches each turn.\n",
    "    Returns \"empty_url_list\", a list of matches_id not assigned to a match.\n",
    "    Also returns a DataFrame with the shots of the matches scraped.\n",
    "    \"\"\"\n",
    "    empty_url_list = []\n",
    "    shots_list = []\n",
    "\n",
    "    for match_id in iterable:\n",
    "        print(match_id)\n",
    "        soup_scripts = scrape_script_tags(match_id)\n",
    "        if len(soup_scripts) == 0:\n",
    "            empty_url_list.append(match_id)\n",
    "        else:\n",
    "            match_shots_list = create_shots_list(soup_scripts)\n",
    "            shots_list.extend(match_shots_list)\n",
    "\n",
    "    return empty_url_list, pd.DataFrame.from_dict(shots_list)\n",
    "\n",
    "\n",
    "def generate_shots_csv(shots_df, file_name):\n",
    "    \"\"\"\n",
    "    Takes a DataFrame of shots and a filename (string ending in .csv).\n",
    "    Exports the DataFrame to a .csv file, creating folders if they don't exist.\n",
    "    \"\"\"\n",
    "    os.makedirs(DIR, exist_ok=True)\n",
    "    file_path = os.path.join(DIR, file_name)\n",
    "\n",
    "    shots_df.to_csv(file_path,index=False)\n",
    "\n",
    "\n",
    "def save_empty_url_list(empty_url_list, file_name):\n",
    "    \"\"\"\n",
    "    Takes a list of matches_id not assigned to a match and a\n",
    "    filename (string ending in .txt).\n",
    "    Exports the list to a .txt file using pickle, creating folders if they\n",
    "    don't exist.\n",
    "    \"\"\"\n",
    "    os.makedirs(DIR_CODE, exist_ok=True)\n",
    "    file_path = os.path.join(DIR_CODE, file_name)\n",
    "\n",
    "    with open(file_path, \"wb\") as fp:\n",
    "        pickle.dump(empty_url_list, fp)\n",
    "\n",
    "\n",
    "def merge_shots_csvs():\n",
    "    \"\"\"\n",
    "    Merges every .csv file with shots data into a single file.\n",
    "    \"\"\"\n",
    "\n",
    "    csv_path = os.path.join(DIR, \"*.csv\")\n",
    "    csv_list = glob.glob(csv_path)\n",
    "    file_path = os.path.join(DIR, \"shots_dataset.csv\")\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for file in csv_list:\n",
    "        df_list.append(pd.read_csv(file))\n",
    "\n",
    "    merged_df = pd.concat(df_list)\n",
    "    merged_df[\"player\"] = merged_df[\"player\"].str.replace(\"&#039;\", \"'\")\n",
    "    merged_df.to_csv(file_path,index=False)\n",
    "\n",
    "\n",
    "def merge_empty_url_lists():\n",
    "    \"\"\"\n",
    "    Reads every \"empty_url_list\" .txt files and saves it into a single file.\n",
    "    \"\"\"\n",
    "\n",
    "    txt_path = os.path.join(DIR_CODE, \"empty*.txt\")\n",
    "    txt_list = glob.glob(txt_path)\n",
    "    file_path = os.path.join(DIR_CODE, \"empty_url.txt\")\n",
    "\n",
    "    empty_url_list = []\n",
    "\n",
    "    for file in txt_list:\n",
    "        with open(file, \"rb\") as fp:\n",
    "            l = pickle.load(fp)\n",
    "        empty_url_list.extend(l)\n",
    "\n",
    "    with open(file_path, \"wb\") as fp:\n",
    "        pickle.dump(empty_url_list, fp)\n",
    "\n",
    "\n",
    "def open_teams_set(teams_file_name):\n",
    "    \"\"\"\n",
    "    Opens a .txt file exported in the season_scraper.py module. Each league\n",
    "    has a file, containing the name of every team that played at least a\n",
    "    season between 2014 and 2020. The names are used to split the shots\n",
    "    datasets in leagues.\n",
    "    \"\"\"\n",
    "\n",
    "    file_path = os.path.join(DIR_TEAMS, teams_file_name)\n",
    "\n",
    "    with open(file_path, \"rb\") as fp:\n",
    "        teams_set = pickle.load(fp)\n",
    "\n",
    "    return teams_set\n",
    "\n",
    "\n",
    "def generate_shots_csvs_by_league_season(first_year, last_year):\n",
    "    \"\"\"\n",
    "    Splits the shots dataset in a different .csv file for each combination of\n",
    "    league/season. Selecting 2014 and 2020 as arguments, results in 42 files\n",
    "    (6 leagues and 7 seasons).\n",
    "    \"\"\"\n",
    "    leagues = [\"EPL\", \"La_liga\", \"Bundesliga\", \"Serie_A\", \"Ligue_1\", \"RFPL\"]\n",
    "    years = list(range(first_year,last_year + 1))\n",
    "\n",
    "    for league in leagues:\n",
    "        for year in years:\n",
    "\n",
    "            league_lower = league.lower()\n",
    "            season_years = str(year)[-2:] + \"-\" + str(year+1)[-2:]\n",
    "\n",
    "            shots_df_path = os.path.join(DIR, \"shots_dataset.csv\")\n",
    "            output_file = \"shots_\" + league_lower + \"_\" + season_years + \".csv\"\n",
    "            output_dir = os.path.join(DIR, league_lower)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            file_path = os.path.join(output_dir, output_file)\n",
    "\n",
    "            teams_set = open_teams_set(league + \"_teams.txt\")\n",
    "            shots_df = pd.read_csv(shots_df_path)\n",
    "            shots_league = shots_df[shots_df[\"h_team\"].isin(teams_set)]\n",
    "            filter_season = shots_league[\"season\"]==year\n",
    "            shots_season = shots_league.where(filter_season).dropna(subset=[\"season\"])\n",
    "            shots_season.to_csv(file_path,index=False)\n",
    "\n",
    "\n",
    "def remove_forgotten_empty_urls():\n",
    "    \"\"\"\n",
    "    This function was used only after the data was initially scraped. There were\n",
    "    thousands of matches_id \"left behind\", ids that will remain unused by\n",
    "    understat.com. This function removes them and keeps only the ids that\n",
    "    will be used in the remaining of the 2020/2021 season.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(DIR_CODE, \"empty_url backup.txt\")\n",
    "    with open(file_path, \"rb\") as fp:\n",
    "        old_empty_url_list = pickle.load(fp)\n",
    "    old_empty_url_list.sort()\n",
    "    update_empty_url = old_empty_url_list[1734:]\n",
    "\n",
    "    save_empty_url_list(update_empty_url, \"empty_url_update.txt\")\n",
    "\n",
    "\n",
    "def update_shots_dataset(year):\n",
    "    \"\"\"\n",
    "    This function updates the datasets. Passing 2020 as argument, the funcion\n",
    "    takes the latest empty_url_update.txt, iterates over it and scrapes the URLs\n",
    "    corresponding to those matches_id. Then, the new data is merged with the\n",
    "    old data and a new empty_url_update.txt is saved, removing the matches scraped.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(DIR_CODE, \"empty_url_update.txt\")\n",
    "    with open(file_path, \"rb\") as fp:\n",
    "        old_empty_url_list = pickle.load(fp)\n",
    "    old_empty_url_list.sort()\n",
    "\n",
    "    empty_url_list, shots_update = generate_shots_dataset(old_empty_url_list)\n",
    "    save_empty_url_list(empty_url_list, \"empty_url_update.txt\")\n",
    "\n",
    "    try:\n",
    "        shots_update[\"player\"] = shots_update[\"player\"].str.replace(\"&#039;\", \"'\")\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    df_list = [shots_update]\n",
    "    shots_file = os.path.join(DIR, \"shots_dataset.csv\")\n",
    "    shots_dataset = pd.read_csv(shots_file)\n",
    "    df_list.append(shots_dataset)\n",
    "    merged_df = pd.concat(df_list)\n",
    "\n",
    "    generate_shots_csv(merged_df, \"shots_dataset.csv\")\n",
    "    generate_shots_csvs_by_league_season(year, year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_shots_dataset(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14224, 14229, 14235, 14249, 14250, 14251, 14252, 14253, 14254, 14255, 14256, 14257, 14258, 14259, 14260, 14261, 14262, 14263, 14264, 14265, 14266, 14267, 14268, 14269, 14270, 14271, 14272, 14273, 14274, 14275, 14276, 14277, 14278, 14279, 14280, 14281, 14282, 14283, 14284, 14285, 14286, 14287, 14288, 14289, 14290, 14291, 14292, 14293, 14294, 14295, 14296, 16136, 16137, 16138, 16139, 16140, 16141, 16142, 16143, 16144, 16145, 16146, 16147, 16148, 16149, 16150, 16151, 16152, 16153, 16154, 16155, 16156, 16157, 16158, 16159, 16160, 16161, 16162, 16163, 16164, 16165, 16166, 16167, 16168, 16169, 16170, 16171, 16172, 16173, 16174, 16175, 16176, 16177, 16178, 16179, 16180, 16181, 16182, 16183, 16184, 16185, 16186, 16187, 16188, 16189, 16190, 16191, 16192, 16193, 16194, 16195, 16196, 16197, 16198, 16199, 16200]\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(DIR_CODE, \"empty_url_update.txt\")\n",
    "\n",
    "with open(file_path, \"rb\") as fp:\n",
    "    old_empty_url_list = pickle.load(fp)\n",
    "    print(old_empty_url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
