{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter the match id:  dd\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-37efe95fd71d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mscripts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'script'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mstrings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscripts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mindex_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"('\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "base_url = 'https://understat.com/match/'\n",
    "match = str(input('Please enter the match id: '))\n",
    "url = base_url+match\n",
    "\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.content, 'lxml')\n",
    "scripts = soup.find_all('script')\n",
    "\n",
    "strings = scripts[1].string\n",
    "\n",
    "index_start = strings.index(\"('\")+2\n",
    "index_end = strings.index(\"')\")\n",
    "json_data = strings[index_start:index_end]\n",
    "json_data = json_data.encode('utf8').decode('unicode_escape')\n",
    "data = json.loads(json_data)\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "minute = []\n",
    "xG = []\n",
    "result = []\n",
    "team = []\n",
    "player = []\n",
    "shotType = []\n",
    "assist = []\n",
    "data_away = data['a']\n",
    "data_home = data['h']\n",
    "\n",
    "for index in range(len(data_home)):\n",
    "    for key in data_home[index]:\n",
    "        if key == 'X':\n",
    "            x.append(data_home[index][key])\n",
    "        if key == 'Y':\n",
    "            y.append(data_home[index][key])\n",
    "        if key == 'minute':\n",
    "            minute.append(data_home[index][key])\n",
    "        if key == 'h_team':\n",
    "            team.append(data_home[index][key])\n",
    "        if key == 'xG':\n",
    "            xG.append(data_home[index][key])\n",
    "        if key == 'result':\n",
    "            result.append(data_home[index][key])\n",
    "        if key == 'player':\n",
    "            player.append(data_home[index][key])\n",
    "        if key == 'player_assisted':\n",
    "            assist.append(data_home[index][key])\n",
    "        if key == 'shotType':\n",
    "            shotType.append(data_home[index][key])\n",
    "\n",
    "for index in range(len(data_away)):\n",
    "    for key in data_away[index]:\n",
    "        if key == 'X':\n",
    "            x.append(data_away[index][key])\n",
    "        elif key == 'Y':\n",
    "            y.append(data_away[index][key])\n",
    "        elif key == 'minute':\n",
    "            minute.append(data_away[index][key])\n",
    "        elif key == 'a_team':\n",
    "            team.append(data_away[index][key])\n",
    "        elif key == 'xG':\n",
    "            xG.append(data_away[index][key])\n",
    "        elif key == 'result':\n",
    "            result.append(data_away[index][key])\n",
    "        elif key == 'player':\n",
    "            player.append(data_away[index][key])\n",
    "        elif key == 'player_assisted':\n",
    "            assist.append(data_away[index][key])\n",
    "        elif key == 'shotType':\n",
    "            shotType.append(data_away[index][key])\n",
    "\n",
    "\n",
    "col_names = [ 'minute', 'team','player', 'x', 'y', 'xG', 'result', 'shotType','assist']\n",
    "df = pd.DataFrame([minute, team, player, x, y, xG ,result ,shotType, assist],index=col_names)\n",
    "df = df.T\n",
    "df = df.astype({'x':float, 'y':float, 'minute':int, 'xG':float,'player':str, 'shotType':str, 'assist':str,'team':str})\n",
    "\n",
    "comp = str(input(\"please input match number.\")) # ex #1\n",
    "season = str(input(\"please input season.\"))\n",
    "team = str(input(\"please input target team.\"))\n",
    "\n",
    "path = f'/work/assets/understats/{season}/{team}/'\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "df.to_csv(path+\"#\"+comp+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://github.com/douglasbc/scraping-understat-dataset/tree/main/scraping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/scrayper/understats\n",
      "/work/scrayper/understats/datasets\n",
      "/work/scrayper/understats/scraping\n",
      "/work/scrayper/understats/scraping/teams_set\n"
     ]
    }
   ],
   "source": [
    "CWD = os.getcwd()\n",
    "DIR = os.path.join(CWD,\"datasets\")\n",
    "DIR_CODE = os.path.join(CWD,\"scraping\")\n",
    "DIR_TEAMS = os.path.join(CWD,\"scraping\", \"teams_set\")\n",
    "print(CWD)\n",
    "print(DIR)\n",
    "print(DIR_CODE)\n",
    "print(DIR_TEAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "CWD = os.getcwd()\n",
    "DIR = os.path.join(CWD,\"datasets\")\n",
    "DIR_CODE = os.path.join(CWD,\"scraping\")\n",
    "DIR_TEAMS = os.path.join(CWD,\"scraping\", \"teams_set\")\n",
    "\n",
    "BASE_URL = \"https://understat.com/match/\"\n",
    "\n",
    "\n",
    "def scrape_script_tags(match_id):\n",
    "    \"\"\"\n",
    "    Takes a match_id (integer) to define a URL to be scraped.\n",
    "    Returns a ResultSet object, class implemented by BeautifulSoup, with all\n",
    "    the script tags in the URL.\n",
    "    \"\"\"\n",
    "    URL = BASE_URL + str(match_id)\n",
    "\n",
    "    response = requests.get(URL)\n",
    "    soup = BeautifulSoup(response.content, \"lxml\")\n",
    "    soup_scripts = soup.find_all(\"script\")\n",
    "    return soup_scripts\n",
    "\n",
    "\n",
    "def create_shots_list(soup_scripts):\n",
    "    \"\"\"\n",
    "    Takes a ResultSet and returns a list of dictionaries containing all shots\n",
    "    in a soccer match.\n",
    "    \"\"\"\n",
    "    shots_string = soup_scripts[1].string\n",
    "    start_index = shots_string.index(\"('\")+2\n",
    "    end_index = shots_string.index(\"')\")\n",
    "    json_string = shots_string[start_index:end_index]\n",
    "    json_string = json_string.encode(\"utf8\").decode(\"unicode_escape\")\n",
    "    shots_dict = json.loads(json_string)\n",
    "\n",
    "    shots_home = shots_dict[\"h\"]\n",
    "    shots_away = shots_dict[\"a\"]\n",
    "\n",
    "    shots_list = shots_home + shots_away\n",
    "    return shots_list\n",
    "\n",
    "\n",
    "def generate_shots_dataset(iterable):\n",
    "    \"\"\"\n",
    "    Takes an iterable object with matches_id. It is passed to a loop that\n",
    "    goes through all matches and scrapes the data. Sometimes a Connection Error\n",
    "    is raised when trying to scrape many matches at once, so might be a good\n",
    "    idea to scrape only a few hundred matches each turn.\n",
    "    Returns \"empty_url_list\", a list of matches_id not assigned to a match.\n",
    "    Also returns a DataFrame with the shots of the matches scraped.\n",
    "    \"\"\"\n",
    "    empty_url_list = []\n",
    "    shots_list = []\n",
    "\n",
    "    for match_id in iterable:\n",
    "        print(match_id)\n",
    "        soup_scripts = scrape_script_tags(match_id)\n",
    "        if len(soup_scripts) == 0:\n",
    "            empty_url_list.append(match_id)\n",
    "        else:\n",
    "            match_shots_list = create_shots_list(soup_scripts)\n",
    "            shots_list.extend(match_shots_list)\n",
    "\n",
    "    return empty_url_list, pd.DataFrame.from_dict(shots_list)\n",
    "\n",
    "\n",
    "def generate_shots_csv(shots_df, file_name):\n",
    "    \"\"\"\n",
    "    Takes a DataFrame of shots and a filename (string ending in .csv).\n",
    "    Exports the DataFrame to a .csv file, creating folders if they don't exist.\n",
    "    \"\"\"\n",
    "    os.makedirs(DIR, exist_ok=True)\n",
    "    file_path = os.path.join(DIR, file_name)\n",
    "\n",
    "    shots_df.to_csv(file_path,index=False)\n",
    "\n",
    "\n",
    "def save_empty_url_list(empty_url_list, file_name):\n",
    "    \"\"\"\n",
    "    Takes a list of matches_id not assigned to a match and a\n",
    "    filename (string ending in .txt).\n",
    "    Exports the list to a .txt file using pickle, creating folders if they\n",
    "    don't exist.\n",
    "    \"\"\"\n",
    "    os.makedirs(DIR_CODE, exist_ok=True)\n",
    "    file_path = os.path.join(DIR_CODE, file_name)\n",
    "\n",
    "    with open(file_path, \"wb\") as fp:\n",
    "        pickle.dump(empty_url_list, fp)\n",
    "\n",
    "\n",
    "def merge_shots_csvs():\n",
    "    \"\"\"\n",
    "    Merges every .csv file with shots data into a single file.\n",
    "    \"\"\"\n",
    "\n",
    "    csv_path = os.path.join(DIR, \"*.csv\")\n",
    "    csv_list = glob.glob(csv_path)\n",
    "    file_path = os.path.join(DIR, \"shots_dataset.csv\")\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for file in csv_list:\n",
    "        df_list.append(pd.read_csv(file))\n",
    "\n",
    "    merged_df = pd.concat(df_list)\n",
    "    merged_df[\"player\"] = merged_df[\"player\"].str.replace(\"&#039;\", \"'\")\n",
    "    merged_df.to_csv(file_path,index=False)\n",
    "\n",
    "\n",
    "def merge_empty_url_lists():\n",
    "    \"\"\"\n",
    "    This function was used only when the data was initially scraped.\n",
    "    Reads every \"empty_url_list\" .txt files and saves it into a single file.\n",
    "    \"\"\"\n",
    "\n",
    "    txt_path = os.path.join(DIR_CODE, \"empty*.txt\")\n",
    "    txt_list = glob.glob(txt_path)\n",
    "    file_path = os.path.join(DIR_CODE, \"empty_url.txt\")\n",
    "\n",
    "    empty_url_list = []\n",
    "\n",
    "    for file in txt_list:\n",
    "        with open(file, \"rb\") as fp:\n",
    "            l = pickle.load(fp)\n",
    "        empty_url_list.extend(l)\n",
    "\n",
    "    with open(file_path, \"wb\") as fp:\n",
    "        pickle.dump(empty_url_list, fp)\n",
    "\n",
    "\n",
    "def open_teams_set(teams_file_name):\n",
    "    \"\"\"\n",
    "    Opens a .txt file exported in the season_scraper.py module. Each league\n",
    "    has a file, containing the name of every team that played at least a\n",
    "    season between 2014 and 2021. The names are used to split the shots\n",
    "    datasets in leagues.\n",
    "    \"\"\"\n",
    "\n",
    "    file_path = os.path.join(DIR_TEAMS, teams_file_name)\n",
    "\n",
    "    with open(file_path, \"rb\") as fp:\n",
    "        teams_set = pickle.load(fp)\n",
    "\n",
    "    return teams_set\n",
    "\n",
    "\n",
    "def generate_shots_csvs_by_league_season(first_year, last_year):\n",
    "    \"\"\"\n",
    "    Splits the shots dataset in a different .csv file for each combination of\n",
    "    league/season. Selecting 2014 and 2021 as arguments, results in 48 files\n",
    "    (6 leagues and 8 seasons).\n",
    "    \"\"\"\n",
    "    leagues = [\"EPL\", \"La_liga\", \"Bundesliga\", \"Serie_A\", \"Ligue_1\", \"RFPL\"]\n",
    "    years = list(range(first_year,last_year + 1))\n",
    "\n",
    "    for league in leagues:\n",
    "        for year in years:\n",
    "\n",
    "            league_lower = league.lower()\n",
    "            season_years = str(year)[-2:] + \"-\" + str(year+1)[-2:]\n",
    "\n",
    "            shots_df_path = os.path.join(DIR, \"shots_dataset.csv\")\n",
    "            output_file = \"shots_\" + league_lower + \"_\" + season_years + \".csv\"\n",
    "            output_dir = os.path.join(DIR, league_lower)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            file_path = os.path.join(output_dir, output_file)\n",
    "\n",
    "            teams_set = open_teams_set(league + \"_teams.txt\")\n",
    "            shots_df = pd.read_csv(shots_df_path)\n",
    "            shots_league = shots_df[shots_df[\"h_team\"].isin(teams_set)]\n",
    "            filter_season = shots_league[\"season\"]==year\n",
    "            shots_season = shots_league.where(filter_season).dropna(subset=[\"season\"])\n",
    "            shots_season.to_csv(file_path,index=False)\n",
    "\n",
    "\n",
    "def remove_forgotten_empty_urls():\n",
    "    \"\"\"\n",
    "    This function was used only after the data was initially scraped. There were\n",
    "    thousands of matches_id \"left behind\", ids that will remain unused by\n",
    "    understat.com. This function removes them and keeps only the ids that\n",
    "    will be used in the remaining of the 2020/2021 season.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(DIR_CODE, \"empty_url backup.txt\")\n",
    "    with open(file_path, \"rb\") as fp:\n",
    "        old_empty_url_list = pickle.load(fp)\n",
    "    old_empty_url_list.sort()\n",
    "    update_empty_url = old_empty_url_list[1734:]\n",
    "\n",
    "    save_empty_url_list(update_empty_url, \"empty_url_update.txt\")\n",
    "\n",
    "\n",
    "def update_shots_dataset(year):\n",
    "    \"\"\"\n",
    "    This function updates the datasets. Passing 2021 as argument, the funcion\n",
    "    takes the latest empty_url_update.txt, iterates over it and scrapes the URLs\n",
    "    corresponding to those matches_id. Then, the new data is merged with the\n",
    "    old data and a new empty_url_update.txt is saved, removing the matches scraped.\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(DIR_CODE, \"empty_url_update.txt\")\n",
    "    with open(file_path, \"rb\") as fp:\n",
    "        old_empty_url_list = pickle.load(fp)\n",
    "    old_empty_url_list.sort()\n",
    "\n",
    "    empty_url_list, shots_update = generate_shots_dataset(old_empty_url_list)\n",
    "    save_empty_url_list(empty_url_list, \"empty_url_update.txt\")\n",
    "\n",
    "    try:\n",
    "        shots_update[\"player\"] = shots_update[\"player\"].str.replace(\"&#039;\", \"'\")\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    df_list = [shots_update]\n",
    "    shots_file = os.path.join(DIR, \"shots_dataset.csv\")\n",
    "    shots_dataset = pd.read_csv(shots_file)\n",
    "    df_list.append(shots_dataset)\n",
    "    merged_df = pd.concat(df_list)\n",
    "\n",
    "    generate_shots_csv(merged_df, \"shots_dataset.csv\")\n",
    "    generate_shots_csvs_by_league_season(year, year)\n",
    "\n",
    "\n",
    "def patch_empty_url_list():\n",
    "    \"\"\"\n",
    "    This function should only be run at the start of a new season.\n",
    "    It should be modified every season, patching the matches_id that\n",
    "    will be used throughout the season.\n",
    "    \"\"\"\n",
    "    new_list = list(range(16136, 18202))\n",
    "    save_empty_url_list(new_list, \"empty_url_update.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2643c9ead153>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# update_shots_dataset(2022)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenerate_shots_csvs_by_league_season\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2021\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2022\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# generate_players_csv(2020,2020)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ed81fe9533eb>\u001b[0m in \u001b[0;36mgenerate_shots_csvs_by_league_season\u001b[0;34m(first_year, last_year)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mteams_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_teams_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleague\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_teams.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mshots_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshots_df_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mshots_league\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshots_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshots_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"h_team\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteams_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mfilter_season\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshots_league\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"season\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_dtype_objs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "# update_shots_dataset(2022)\n",
    "generate_players_csv(2020,2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python /work/scrayper/understats/get_player_data.py \"Osasuna\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
